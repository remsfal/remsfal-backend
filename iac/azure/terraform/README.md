# Infrastructure as Code - Azure Terraform

Diese Dokumentation beschreibt die Terraform-basierte Infrastruktur f√ºr die REMSFAL-Anwendung auf Microsoft Azure.

**Repository:** [GitHub - IaC Terraform](https://github.com/enricogoerlitz/remsfal-backend/tree/Enrico-Goerlitz%23644/iac/azure/terraform)

---

## Inhaltsverzeichnis

1. [Projektstruktur](#projektstruktur)
2. [Verwendete Versionen](#verwendete-versionen)
3. [Naming Convention](#naming-convention)
4. [Azure-Ressourcen im √úberblick](#azure-ressourcen-im-√ºberblick)
5. [Detaillierte Ressourcen-Konfiguration](#detaillierte-ressourcen-konfiguration)
6. [Umgebungskonfiguration](#umgebungskonfiguration)
7. [Secret Management](#secret-management)
8. [Managed Identity & RBAC](#managed-identity--rbac)

---

## Projektstruktur

```
iac/azure/terraform/
‚îú‚îÄ‚îÄ main.tf              # Hauptkonfiguration aller Azure-Ressourcen
‚îú‚îÄ‚îÄ variables.tf         # Variablendefinitionen mit Validierung
‚îú‚îÄ‚îÄ locals.tf            # Lokale Werte und Naming-Logik
‚îú‚îÄ‚îÄ outputs.tf           # Output-Werte nach dem Deployment
‚îú‚îÄ‚îÄ providers.tf         # Provider-Konfiguration und Backend
‚îú‚îÄ‚îÄ README.md            # Terraform-spezifische Dokumentation
‚îú‚îÄ‚îÄ env/                 # Umgebungsspezifische Konfigurationen
‚îÇ   ‚îú‚îÄ‚îÄ dev.tfvars       # Development-Umgebung
‚îÇ   ‚îú‚îÄ‚îÄ tst.tfvars       # Test-Umgebung
‚îÇ   ‚îî‚îÄ‚îÄ prd.tfvars       # Production-Umgebung
‚îî‚îÄ‚îÄ scripts/             # Hilfs-Skripte
```

### Dateibeschreibungen

| Datei | Beschreibung |
|-------|--------------|
| `main.tf` | Definiert alle Azure-Ressourcen: Container Apps, Datenbanken, Event Hubs, Key Vault, Storage, Monitoring |
| `variables.tf` | Deklariert alle Eingabevariablen mit Typen, Beschreibungen und Standardwerten |
| `locals.tf` | Berechnet abgeleitete Werte wie Ressourcennamen, Tags und Cosmos DB Tabellenschemata |
| `outputs.tf` | Exportiert wichtige Werte nach dem Deployment (URLs, Connection Strings, Principal IDs) |
| `providers.tf` | Konfiguriert Azure Provider und Remote Backend f√ºr State-Management |

---

## Verwendete Versionen

### Terraform

| Komponente | Version | Beschreibung |
|------------|---------|--------------|
| **Terraform** | >= 1.5 | Infrastructure as Code Tool |
| **azurerm Provider** | ~> 4.57.0 | Azure Resource Manager Provider |
| **azapi Provider** | ~> 2.8.0 | Azure API Direct Access Provider |
| **random Provider** | ~> 3.6 | Zufallsgenerierung f√ºr eindeutige Namen |

### Backend-Konfiguration

Der Terraform State wird remote in Azure Blob Storage gespeichert:

```hcl
backend "azurerm" {
  resource_group_name  = "remsfal-iac-rg"
  storage_account_name = "engobaremsfalsa"
  container_name       = "tfstate"
  key                  = "terraform.tfstate"
}
```

**Begr√ºndung:** Remote State erm√∂glicht Team-Kollaboration und verhindert State-Konflikte. Azure Blob Storage bietet integrierte Versionierung und Locking.

---

## Naming Convention

Alle Ressourcen folgen einem einheitlichen Namensschema:

```
{project_name_short}-{environment}-{location_short}-{resource_short}
```

**Beispiel:** `rmsfl-dev-weu-rg` (Resource Group f√ºr Development in West Europe)

### Namenszusammensetzung

| Komponente | Wert | Beschreibung |
|------------|------|--------------|
| `project_name_short` | `rmsfl` | Kurzform von "remsfal" |
| `environment` | `dev`, `tst`, `prd` | Umgebungsbezeichnung |
| `location_short` | `weu` | West Europe |
| `resource_short` | `rg`, `acr`, `kv`, etc. | Ressourcentyp-K√ºrzel |

### Sonderregeln

- **Azure Container Registry (ACR):** Keine Bindestriche erlaubt ‚Üí `rmsfldevweuacr`
- **Storage Account:** Keine Bindestriche, max. 24 Zeichen ‚Üí `rmsfldevweusa`
- **Key Vault:** Eindeutiger Name erforderlich ‚Üí Suffix mit `random_string`

---

## Azure-Ressourcen im √úberblick

### Architekturdiagramm

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           Azure Resource Group                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ                    Container Apps Environment                       ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Frontend ‚îÇ ‚îÇ Platform ‚îÇ ‚îÇTicketing ‚îÇ ‚îÇ  OCR     ‚îÇ ‚îÇNotific.  ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ (Vue.js) ‚îÇ ‚îÇ (Quarkus)‚îÇ ‚îÇ(Quarkus) ‚îÇ ‚îÇ (Python) ‚îÇ ‚îÇ(Quarkus) ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Port 80  ‚îÇ ‚îÇ Port 8080‚îÇ ‚îÇPort 8081 ‚îÇ ‚îÇPort 8000 ‚îÇ ‚îÇPort 8082 ‚îÇ   ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ          ‚îÇ            ‚îÇ            ‚îÇ            ‚îÇ            ‚îÇ              ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ                         Azure Key Vault                             ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ         (Secrets: DB Credentials, Event Hub, Storage)               ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ   PostgreSQL    ‚îÇ  ‚îÇ    Cosmos DB    ‚îÇ  ‚îÇ     Azure Event Hubs      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ Flexible Server ‚îÇ  ‚îÇ  (Cassandra API)‚îÇ  ‚îÇ    (Kafka-kompatibel)     ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   (Platform)    ‚îÇ  ‚îÇ   (Ticketing)   ‚îÇ  ‚îÇ Topics: user-notif.,      ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ ocr.documents.*           ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ  ‚îÇ  Blob Storage   ‚îÇ  ‚îÇ Container       ‚îÇ  ‚îÇ   Application Insights    ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ  (Documents,    ‚îÇ  ‚îÇ Registry (ACR)  ‚îÇ  ‚îÇ   + Log Analytics         ‚îÇ    ‚îÇ
‚îÇ  ‚îÇ   Attachments)  ‚îÇ  ‚îÇ                 ‚îÇ  ‚îÇ                           ‚îÇ    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Ressourcen√ºbersicht

| Ressource | Azure Service | Zweck |
|-----------|---------------|-------|
| **Compute** | Container Apps | Hosting der 5 Microservices |
| **Container Images** | Container Registry (ACR) | Private Docker Registry |
| **Relationale DB** | PostgreSQL Flexible Server | Platform-Service Datenbank |
| **NoSQL DB** | Cosmos DB (Cassandra API) | Ticketing-Service Datenbank |
| **Messaging** | Event Hubs | Kafka-kompatibler Message Broker |
| **Object Storage** | Blob Storage | Dokumente und Attachments |
| **Secrets** | Key Vault | Credentials und Connection Strings |
| **Monitoring** | Application Insights | APM und Distributed Tracing |
| **Logs** | Log Analytics Workspace | Zentralisierte Log-Aggregation |

---

## Detaillierte Ressourcen-Konfiguration

### 1. Container Apps Environment

**Ressource:** `azurerm_container_app_environment`

Das Container Apps Environment ist die serverlose Hosting-Plattform f√ºr alle Microservices.

```hcl
resource "azurerm_container_app_environment" "main" {
  name                       = local.container_apps_environment_name
  resource_group_name        = azurerm_resource_group.main.name
  location                   = azurerm_resource_group.main.location
  log_analytics_workspace_id = azurerm_log_analytics_workspace.main.id
}
```

**Begr√ºndung:**
- **Serverless:** Automatische Skalierung basierend auf Last
- **Integriertes Logging:** Direkte Anbindung an Log Analytics
- **KEDA-Integration:** Event-driven Autoscaling f√ºr Kafka-Consumer

### 2. Container Apps (Microservices)

**Ressource:** `azurerm_container_app`

Jeder Microservice wird als eigene Container App deployt. Die Konfiguration verwendet eine **User-Assigned Managed Identity** f√ºr ACR-Zugriff und eine **System-Assigned Identity** f√ºr weitere Ressourcenzugriffe.

#### Ressourcen-Konfiguration (Development)

| Service | CPU | Memory | Min Replicas | Max Replicas | Ingress | Port |
|---------|-----|--------|--------------|--------------|---------|------|
| **Frontend** | 0.25 | 0.5 Gi | 0 | 2 | External | 80 |
| **Platform** | 0.25 | 0.5 Gi | 0 | 3 | External | 8080 |
| **Ticketing** | 0.25 | 0.5 Gi | 0 | 3 | External | 8081 |
| **Notification** | 0.25 | 0.5 Gi | 0 | 2 | Internal | 8082 |
| **OCR** | 0.5 | 1 Gi | 0 | 2 | Kein Ingress | 8000 |

**Konfigurationsentscheidungen:**

- **min_replicas=0 (Scale-to-Zero):** Alle Services in Dev/Test skalieren auf 0 bei Inaktivit√§t. Dies spart erhebliche Kosten, f√ºhrt aber zu **Cold-Start-Latenz** von ca. 10-30 Sekunden beim ersten Request.
- **Minimale Ressourcen:** CPU=0.25 und Memory=0.5Gi sind die Minimalwerte f√ºr Container Apps. Der OCR-Service ben√∂tigt mehr Ressourcen (0.5 CPU, 1Gi Memory) wegen der ML-Modelle.
- **OCR ohne HTTP-Ingress:** Der OCR-Service kommuniziert ausschlie√ülich √ºber Event Hubs (Kafka), daher kein HTTP-Ingress erforderlich.

> **‚ö†Ô∏è Production-Empfehlung:** F√ºr kritische Services (Platform, Ticketing) sollte `min_replicas >= 1` gesetzt werden, um Cold-Starts zu vermeiden und die Verf√ºgbarkeit zu gew√§hrleisten.

#### KEDA Event Hub Scaling (OCR-Service)

```hcl
eventhub_scaling = {
  enabled               = true
  consumer_group        = "ocr-service"  # Dedizierte Consumer Group!
  event_hub_name        = "ocr.documents.to_process"
  message_lag_threshold = 10
}
```

**Begr√ºndung:** Der OCR-Service skaliert automatisch basierend auf der Anzahl unverarbeiteter Nachrichten in der Event Hub Queue. Bei 10 oder mehr wartenden Dokumenten wird eine zus√§tzliche Instanz gestartet.

**Wichtig zur Consumer Group:**
- **NICHT `$Default` verwenden!** Die `$Default` Consumer Group ist f√ºr allgemeine Zwecke reserviert.
- Dedizierte Consumer Group `ocr-service` gew√§hrleistet korrekte Offset-Verfolgung und verhindert Konflikte mit anderen Konsumenten.
- Die Consumer Group wird automatisch via Terraform auf den Topics `ocr.documents.to_process` und `ocr.documents.processed` erstellt.

### 3. Azure Database for PostgreSQL Flexible Server

**Ressource:** `azurerm_postgresql_flexible_server`

```hcl
resource "azurerm_postgresql_flexible_server" "main" {
  name                   = local.postgres_server_name
  version                = "16"
  administrator_login    = var.postgres_admin_username
  administrator_password = var.postgres_admin_password
  storage_mb             = var.postgres_storage_mb  # 32 GB
  sku_name               = var.postgres_sku         # B_Standard_B1ms
  backup_retention_days  = 7
  zone                   = "1"
  
  authentication {
    active_directory_auth_enabled = true
    password_auth_enabled         = true
  }
}
```

**Konfigurationsentscheidungen:**

| Einstellung | Wert | Begr√ºndung |
|-------------|------|------------|
| **Version** | 16 | Neueste PostgreSQL LTS-Version |
| **SKU** | B_Standard_B1ms | Burstable Tier - kosteng√ºnstig f√ºr Dev/Test |
| **Storage** | 32 GB | Minimum f√ºr Flexible Server |
| **Backup Retention** | 7 Tage | Standard f√ºr automatische Backups |
| **Zone** | 1 | Keine Zone Redundancy f√ºr Kostenoptimierung |
| **AD Auth** | Aktiviert | Erm√∂glicht Managed Identity Authentifizierung |

#### Verf√ºgbare PostgreSQL SKUs

| SKU | vCores | RAM | Tier | Empfohlen f√ºr |
|-----|--------|-----|------|---------------|
| **B_Standard_B1ms** | 1 | 2 GB | Burstable | Development, minimale Kosten |
| **B_Standard_B2s** | 2 | 4 GB | Burstable | Test, leichte Last |
| **B_Standard_B2ms** | 2 | 8 GB | Burstable | Test mit mehr Memory |
| **GP_Standard_D2s_v3** | 2 | 8 GB | General Purpose | Production (empfohlen) |
| **GP_Standard_D4s_v3** | 4 | 16 GB | General Purpose | Production, h√∂here Last |
| **GP_Standard_D8s_v3** | 8 | 32 GB | General Purpose | Production, hohe Last |

> **üí° Hinweis:** Burstable SKUs (B_*) sind f√ºr variable Workloads konzipiert und g√ºnstiger. General Purpose (GP_*) bieten konsistente Performance f√ºr Production.

**Firewall-Regeln:**
- `allow-azure-services`: Erlaubt Zugriff von Azure-Services (Container Apps)
- `allow-all` (nur Dev): Erlaubt externen Zugriff f√ºr Debugging

### 4. Azure Cosmos DB mit Cassandra API

**Ressource:** `azurerm_cosmosdb_account`

```hcl
resource "azurerm_cosmosdb_account" "main" {
  name       = local.cosmos_account_name
  offer_type = "Standard"
  kind       = "GlobalDocumentDB"

  capabilities {
    name = "EnableCassandra"
  }

  consistency_policy {
    consistency_level = "Session"
  }
  
  geo_location {
    location          = azurerm_resource_group.main.location
    failover_priority = 0
  }
}
```

#### Warum Cosmos DB mit Cassandra API?

- **Migration ohne Code-√Ñnderungen:** Der Ticketing-Service wurde urspr√ºnglich f√ºr Apache Cassandra entwickelt. Die Cassandra API erm√∂glicht die Migration ohne √Ñnderungen am Datenmodell oder CQL-Queries.
- **Managed Service:** Kein Betrieb von Cassandra-Clustern erforderlich, automatische Patches und Backups.
- **Globale Verteilung:** Multi-Region Replikation m√∂glich (nicht in dieser Konfiguration aktiviert).

#### Throughput-Konfiguration (Request Units)

| Umgebung | Throughput | Beschreibung |
|----------|------------|--------------|
| **Dev** | 400 RU/s | Minimum f√ºr Cassandra API |
| **Test** | 400 RU/s | Ausreichend f√ºr Integrationstests |
| **Prod** | 1000 RU/s | H√∂here Kapazit√§t f√ºr Production |

> **üí° Request Units (RU/s):** Eine RU entspricht ungef√§hr einer Leseoperation f√ºr ein 1KB-Dokument. Komplexere Operationen (Writes, Queries) verbrauchen mehr RUs.

#### Manual vs. Autoscale Throughput

Die aktuelle Konfiguration verwendet **Manual (Provisioned) Throughput**. Alternativ bietet Cosmos DB **Autoscale**, das automatisch zwischen 10% und 100% des konfigurierten Maximums skaliert.

| Aspekt | Manual (Provisioned) | Autoscale |
|--------|---------------------|-----------|
| **Minimum** | 400 RU/s (Cassandra API) | 10% des Max-Werts (z.B. 100 bei max 1000) |
| **Preis pro RU/s** | 1√ó Basispreis | ~1,5√ó Basispreis (50% teurer) |
| **Abrechnung** | Immer volle RU/s | Nur genutzter Anteil |
| **Skalierungsbereich** | Fest | Automatisch 10%-100% |

**Wann lohnt sich Autoscale?**

Autoscale kann g√ºnstiger sein, obwohl der Preis pro RU/s h√∂her ist:
- **Bei stark variierender Last:** Wenn nachts/am Wochenende kaum Nutzung erfolgt
- **Faustregel:** Autoscale ist g√ºnstiger, wenn durchschnittliche Nutzung **< 66%** der provisionierten RU/s liegt

**Beispielrechnung:**
- **Manual 400 RU/s:** Zahlt immer f√ºr 400 RU/s (100%)
- **Autoscale max 1000 RU/s:** Skaliert zwischen 100-1000 RU/s
  - Bei 10% Last (Nacht): 100 RU/s √ó 1,5 = Kosten wie 150 RU/s manual
  - Bei Peaks: Bis zu 1000 RU/s verf√ºgbar

> **üí° Empfehlung f√ºr Dev/Test:** Bei sporadischer Nutzung (Entwicklung, gelegentliche Tests) kann Autoscale g√ºnstiger sein, da au√üerhalb der Arbeitszeiten nur minimale Kosten anfallen. F√ºr Production mit konstanter Last ist Manual oft kosteneffizienter.

#### Consistency Level

**Session Consistency** wurde gew√§hlt, da sie den besten Kompromiss bietet:
- **Monotonic Reads:** Ein Client sieht nie √§ltere Daten als zuvor gelesene.
- **Read Your Writes:** Eigene Schreiboperationen sind sofort lesbar.
- **Performance:** Geringere Latenz als Strong Consistency.

Andere verf√ºgbare Level: `Eventual`, `ConsistentPrefix`, `BoundedStaleness`, `Strong`

#### Tabellenschema und Partition Keys

```hcl
# Definiert in locals.tf
cosmos_tables = {
  issues = {
    schema = {
      columns = [
        { name = "project_id", type = "uuid" },
        { name = "issue_id", type = "uuid" },
        { name = "title", type = "text" },
        # ... weitere Spalten
      ]
      partition_keys = [{ name = "project_id" }]
      cluster_keys   = [{ name = "issue_id", order_by = "asc" }]
    }
  }
  chat_sessions = {
    schema = {
      partition_keys = [{ name = "project_id" }, { name = "issue_id" }]
      cluster_keys   = [{ name = "session_id", order_by = "asc" }]
    }
  }
  chat_messages = {
    schema = {
      partition_keys = [{ name = "session_id" }]
      cluster_keys   = [{ name = "created_at", order_by = "desc" }]
    }
  }
}
```

**Begr√ºndung f√ºr Partition Key Design:**

| Tabelle | Partition Key | Begr√ºndung |
|---------|---------------|------------|
| **issues** | `project_id` | Alle Issues eines Projekts werden zusammen gespeichert. Erm√∂glicht effiziente Abfragen wie "Alle Issues f√ºr Projekt X". |
| **chat_sessions** | `project_id` + `issue_id` | Composite Key, da Chat-Sessions immer im Kontext eines Issues abgefragt werden. |
| **chat_messages** | `session_id` | Nachrichten werden immer pro Session geladen. `created_at` als Cluster Key erm√∂glicht chronologische Sortierung. |

> **‚ö†Ô∏è Wichtiger Hinweis:** Cosmos DB Cassandra API unterst√ºtzt **keine Managed Identity**. Die Authentifizierung erfolgt √ºber Username/Password, die im Key Vault gespeichert werden. Dies erfordert SSL-Konfiguration im CassandraExecutor (siehe [REFACTORING.md](REFACTORING.md#cosmos-db-cassandra-api-ssl-konfiguration)).

### 5. Azure Event Hubs (Kafka-Ersatz)

**Ressource:** `azurerm_eventhub_namespace`

```hcl
resource "azurerm_eventhub_namespace" "main" {
  name     = local.eventhub_namespace_name
  sku      = "Standard"
  capacity = var.eventhub_capacity  # 1 TU
}
```

> **‚ö†Ô∏è WICHTIG:** F√ºr **Kafka-Kompatibilit√§t ist mindestens Standard SKU erforderlich**! Die REMSFAL-Services verwenden das Kafka-Protokoll (SASL_SSL, Port 9093), daher ist Basic SKU keine Option.

#### Throughput Units (TU)

| Umgebung | TUs | Kapazit√§t |
|----------|-----|-----------|
| **Dev/Test** | 1 | 1 MB/s ingress, 2 MB/s egress |
| **Production** | 2 | 2 MB/s ingress, 4 MB/s egress |

**Konfigurierte Topics (Event Hubs):**

| Topic | Partitions | Retention | Zweck |
|-------|------------|-----------|-------|
| `user-notification` | 2 | 1 Tag | Benutzerbenachrichtigungen |
| `ocr.documents.to_process` | 2 | 1 Tag | Eingangswarteschlange f√ºr OCR |
| `ocr.documents.processed` | 2 | 1 Tag | Verarbeitete OCR-Ergebnisse |

**Begr√ºndung f√ºr Event Hubs statt Kafka:**
- **Vollst√§ndig Kafka-kompatibel:** SASL_SSL, Port 9093, Standard Kafka-Clients funktionieren
- **Managed Service:** Kein Betrieb von Kafka-Clustern/Zookeeper n√∂tig
- **Automatische Skalierung:** Throughput Units je nach Bedarf
- **Native Integration:** KEDA-Scaler f√ºr Container Apps, Azure Functions Trigger

**Consumer Groups:**

```hcl
resource "azurerm_eventhub_consumer_group" "ocr_service" {
  for_each = toset([
    "ocr.documents.to_process",
    "ocr.documents.processed"
  ])
  name           = "ocr-service"
  eventhub_name  = each.value
  namespace_name = azurerm_eventhub_namespace.main.name
}
```

- `ocr-service`: Dedizierte Consumer Group f√ºr den OCR-Service
- `$Default`: Standard Consumer Group (f√ºr andere Services/Debugging)

### 6. Azure Blob Storage

**Ressource:** `azurerm_storage_account`

```hcl
resource "azurerm_storage_account" "main" {
  name                     = local.storage_account_name
  account_tier             = "Standard"
  account_replication_type = "LRS"
  account_kind             = "StorageV2"
}
```

**Konfigurierte Container:**

| Container | Zweck |
|-----------|-------|
| `remsfal-ticketing` | Dokumente f√ºr Ticketing-Service |
| `documents` | Allgemeine Dokumentenspeicherung |
| `test-bucket` | Test-Daten |
| `eventhub-checkpoints` | KEDA Checkpoint Storage |

**Begr√ºndung f√ºr LRS (Locally Redundant Storage):**
- Kostenoptimierung f√ºr Dev/Test-Umgebungen
- F√ºr Production empfohlen: ZRS oder GRS

### 7. Azure Key Vault

**Ressource:** `azurerm_key_vault`

```hcl
resource "azurerm_key_vault" "main" {
  name                       = local.key_vault_name
  tenant_id                  = data.azurerm_client_config.current.tenant_id
  sku_name                   = "standard"
  soft_delete_retention_days = 7
  purge_protection_enabled   = false
  rbac_authorization_enabled = true
}
```

**Gespeicherte Secrets:**

| Secret | Beschreibung |
|--------|--------------|
| `postgres-connection-string` | JDBC Connection String f√ºr PostgreSQL |
| `storage-connection-string` | Azure Storage Connection String |
| `cosmos-contact-point` | Cassandra Contact Point (Host:Port) |
| `cosmos-username` | Cosmos DB Account Name |
| `cosmos-password` | Cosmos DB Primary Key |
| `eventhub-connection-string` | JAAS-Format f√ºr Kafka-Konfiguration |
| `eventhub-sasl-username` | `$ConnectionString` |
| `eventhub-sasl-password` | Event Hub Connection String |
| `eventhub-bootstrap-server` | Kafka Bootstrap Server URL |

**Begr√ºndung f√ºr RBAC:**
- RBAC statt Access Policies erm√∂glicht feinere Zugriffssteuerung
- Konsistent mit Azure-weitem Identity Management

### 8. Monitoring (Application Insights & Log Analytics)

**Ressourcen:** `azurerm_application_insights`, `azurerm_log_analytics_workspace`

```hcl
resource "azurerm_log_analytics_workspace" "main" {
  name              = local.log_analytics_workspace_name
  sku               = "PerGB2018"
  retention_in_days = 30
}

resource "azurerm_application_insights" "main" {
  name             = local.application_insights_name
  workspace_id     = azurerm_log_analytics_workspace.main.id
  application_type = "web"
}
```

**Begr√ºndung:**
- **Log Analytics Workspace:** Zentrale Log-Aggregation f√ºr alle Services
- **Application Insights:** APM mit Distributed Tracing, Dependency Mapping
- **30 Tage Retention:** Ausreichend f√ºr Debugging, kostenoptimiert

---

## Umgebungskonfiguration

Die Infrastruktur unterst√ºtzt drei Umgebungen, die sich in Ressourcenausstattung, Skalierung und Kosten unterscheiden:

| Umgebung | Datei | Beschreibung |
|----------|-------|--------------|
| **Development** | `env/dev.tfvars` | Minimale Ressourcen, Scale-to-Zero, g√ºnstigste Konfiguration |
| **Test** | `env/tst.tfvars` | Mittlere Ressourcen f√ºr Integrationstests und QA |
| **Production** | `env/prd.tfvars` | Hochverf√ºgbarkeit, ausreichende Ressourcen f√ºr Produktionslast |

### Umgebungsvergleich

| Einstellung | Dev | Test | Production |
|-------------|-----|------|------------|
| **PostgreSQL SKU** | B_Standard_B1ms | B_Standard_B2s | GP_Standard_D2s_v3 |
| **PostgreSQL Storage** | 32 GB | 64 GB | 128 GB |
| **Cosmos Throughput** | 400 RU/s | 400 RU/s | 1000 RU/s |
| **Event Hub TUs** | 1 | 1 | 2 |
| **Container min_replicas** | 0 (alle) | 0 (alle) | 1-2 (kritische) |
| **Container max_replicas** | 2-3 | 3-5 | 5-10 |
| **Container CPU** | 0.25-0.5 | 0.25-0.5 | 0.5-1.0 |
| **Container Memory** | 0.5-1 Gi | 0.5-1 Gi | 1-2 Gi |
| **Image Tag** | `:latest` | `:latest` | `:stable` |
| **Storage Redundancy** | LRS | LRS | LRS (GRS empfohlen) |

### Development-Umgebung (dev.tfvars)

**Ziel:** Minimale Kosten bei Entwicklung und lokalem Testing.

```hcl
# Alle Services: Scale-to-Zero f√ºr Kostenoptimierung
# CPU: 0.25 (Minimum), Memory: 0.5Gi (Minimum)
# OCR: 0.5 CPU, 1Gi wegen ML-Modellen

container_apps = {
  platform = {
    cpu          = 0.25
    memory       = "0.5Gi"
    min_replicas = 0  # Scale-to-Zero ‚Üí Cold-Start bei erstem Request
    max_replicas = 3
  }
  # ...
}
```

**Eigenschaften:**
- ‚úÖ G√ºnstigste Konfiguration
- ‚úÖ Scale-to-Zero spart Kosten bei Inaktivit√§t
- ‚ö†Ô∏è Cold-Start-Latenz: 10-30 Sekunden beim ersten Request
- ‚ö†Ô∏è Minimale Ressourcen k√∂nnen bei komplexen Operationen langsam sein

### Test-Umgebung (tst.tfvars)

**Ziel:** Realistische Umgebung f√ºr Integrationstests und QA.

```hcl
# Gleiche Ressourcen wie Dev, aber h√∂here max_replicas f√ºr Lasttests
# PostgreSQL: B_Standard_B2s f√ºr mehr Performance bei parallelen Tests

postgres_sku = "B_Standard_B2s"  # 2 vCores, 4GB RAM

container_apps = {
  platform = {
    min_replicas = 0
    max_replicas = 5  # H√∂her f√ºr Lasttests
  }
}
```

**Eigenschaften:**
- ‚úÖ Gr√∂√üere PostgreSQL-Instanz f√ºr parallele Tests
- ‚úÖ H√∂here max_replicas f√ºr Skalierungstests
- ‚úÖ Scale-to-Zero f√ºr Kostenoptimierung au√üerhalb der Testzeiten
- ‚ö†Ô∏è Cold-Start bei Tests nach l√§ngerer Inaktivit√§t

### Production-Umgebung (prd.tfvars)

**Ziel:** Hochverf√ºgbarkeit und konsistente Performance.

```hcl
# Kritische Services: min_replicas > 0 f√ºr Verf√ºgbarkeit
# Image-Tag: :stable f√ºr kontrollierte Deployments

container_apps = {
  platform = {
    image        = "remsfal-platform:stable"
    cpu          = 1.0
    memory       = "2Gi"
    min_replicas = 2  # Hochverf√ºgbar
    max_replicas = 10
  }
}

postgres_sku = "GP_Standard_D2s_v3"  # General Purpose f√ºr konsistente Performance
cosmos_throughput = 1000              # H√∂here Kapazit√§t
eventhub_capacity = 2                 # Mehr Durchsatz
```

**Eigenschaften:**
- ‚úÖ Keine Cold-Starts f√ºr kritische Services (Platform, Ticketing)
- ‚úÖ Ausreichende Ressourcen f√ºr Produktionslast
- ‚úÖ General Purpose PostgreSQL f√ºr konsistente Performance
- ‚úÖ `:stable` Image-Tags f√ºr kontrollierte Releases
- üí∞ H√∂here Kosten, aber notwendig f√ºr Produktionsbetrieb

> **üí° Empfehlung f√ºr Production:** Zus√§tzlich Zone Redundancy f√ºr PostgreSQL aktivieren und Storage Redundancy auf GRS (Geo-Redundant) √§ndern.

---

## Secret Management

### AzureKeyVaultConfigSource

Die Quarkus-Services laden Secrets direkt aus dem Key Vault mittels der `AzureKeyVaultConfigSource`:

```java
// Quarkus l√§dt automatisch Secrets aus Key Vault
// wenn AZURE_KEYVAULT_ENDPOINT gesetzt ist
@ConfigProperty(name = "quarkus.datasource.jdbc.url")
String jdbcUrl;  // Wird aus Key Vault Secret "postgres-connection-string" geladen
```

**Vorteile:**
- Keine Secrets in Environment Variables oder Configs
- Automatische Rotation m√∂glich
- Zentrale Secret-Verwaltung

### Secret-Mapping f√ºr Services

| Service | Ben√∂tigte Secrets |
|---------|-------------------|
| **Platform** | `postgres-connection-string` |
| **Ticketing** | `cosmos-*`, `storage-connection-string` |
| **Notification** | `eventhub-*` |
| **OCR** | `eventhub-*`, `storage-connection-string` |

---

## Managed Identity & RBAC

### Dual Identity Pattern

Die Container Apps verwenden ein **Dual Identity Pattern** mit zwei verschiedenen Managed Identities:

```hcl
# User-Assigned Identity f√ºr ACR Pull (erstellt VOR den Container Apps)
resource "azurerm_user_assigned_identity" "container_apps" {
  name = "${local.base_name}-ca-identity"
}

# Grant ACR Pull BEVOR Container Apps erstellt werden
resource "azurerm_role_assignment" "container_apps_acr_pull" {
  scope                = data.azurerm_container_registry.main.id
  role_definition_name = "AcrPull"
  principal_id         = azurerm_user_assigned_identity.container_apps.principal_id
}

# Container App mit beiden Identity-Typen
resource "azurerm_container_app" "apps" {
  identity {
    type         = "SystemAssigned, UserAssigned"
    identity_ids = [azurerm_user_assigned_identity.container_apps.id]
  }
  
  # User-Assigned Identity f√ºr ACR Pull
  registry {
    identity = azurerm_user_assigned_identity.container_apps.id
    server   = data.azurerm_container_registry.main.login_server
  }
}
```

**Begr√ºndung f√ºr Dual Identity:**

| Identity-Typ | Verwendungszweck | Warum? |
|--------------|------------------|--------|
| **User-Assigned** | ACR Pull | Identity existiert bevor Container App erstellt wird |
| **System-Assigned** | Storage, Event Hubs, Key Vault | Automatisch verwaltet, pro Container App eindeutig |

### Zugewiesene Rollen

| Rolle | Scope | Identity | Zweck |
|-------|-------|----------|-------|
| `AcrPull` | Container Registry | **User-Assigned** | Container Images aus ACR ziehen |
| `Storage Blob Data Contributor` | Storage Account | System-Assigned | Lesen/Schreiben von Blobs |
| `Azure Event Hubs Data Owner` | Event Hub Namespace | System-Assigned | Kafka Produce/Consume |
| `Key Vault Secrets User` | Key Vault | System-Assigned | Secrets aus Key Vault lesen |

**Vorteile von Managed Identity:**
- Keine Credentials im Code oder Config
- Automatische Credential-Rotation
- Zentrale Zugriffskontrolle √ºber Azure RBAC
- Kein manuelles Secret-Management f√ºr Azure-Services

> **‚ö†Ô∏è Ausnahme Cosmos DB:** Cosmos DB Cassandra API unterst√ºtzt keine Managed Identity. Credentials werden aus Key Vault gelesen.

---

## Hinweise

> **‚ö†Ô∏è Produktionsempfehlung:** Diese Dokumentation beschreibt die IaC-Konfiguration f√ºr Development/Test. F√ºr Production sollten zus√§tzliche Ma√ünahmen wie Zone Redundancy, Geo-Replikation und erweiterte Backup-Strategien implementiert werden.

> **üìù Repository-Hinweis:** Idealerweise sollte Infrastructure as Code in einem eigenen Repository verwaltet werden. Da f√ºr die REMSFAL GitHub-Organisation keine neuen Repositories erstellt werden k√∂nnen, befindet sich der IaC-Code im Backend-Repository unter `/iac/azure/terraform`.
